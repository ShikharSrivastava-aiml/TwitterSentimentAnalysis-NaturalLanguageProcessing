{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data Loading"
   ],
   "id": "a6c2662c8fb7f8b6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a47f1ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a47f1ef",
    "outputId": "2a696590-d694-42ed-91b9-92b8d729afa7"
   },
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7daDN0eKM7bH",
   "metadata": {
    "id": "7daDN0eKM7bH"
   },
   "source": [
    "df = pd.read_csv(\"twitter_training.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4bcf9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "6c4bcf9c",
    "outputId": "7f56c1d6-6099-412d-eba7-24ae70f063d2",
    "scrolled": true
   },
   "source": [
    "# Label the columns\n",
    "df.columns = [\"tweet_id\",\"place\", \"sentiments\", \"tweets\"]\n",
    "df = df.drop(\"place\", axis=1)\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb5568a",
   "metadata": {
    "id": "4fb5568a"
   },
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21afdf30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21afdf30",
    "outputId": "b34a53f6-102b-4c67-d20f-b8828ffeefc0"
   },
   "source": [
    "df.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2228d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "3d2228d0",
    "outputId": "2336c029-cd59-4ddb-d44c-4591b83fe5f6",
    "scrolled": true
   },
   "source": [
    "# Identify inconsistencies\n",
    "#The code `duplicate_rows = df[df.duplicated()]` is identifying and storing the duplicate rows in the DataFrame `df`. The `df.duplicated()` function returns a boolean Series indicating whether each row is a duplicate or not. By passing this boolean Series as a filter to the DataFrame `df`, only the duplicate rows are selected and stored in the variable `duplicate_rows`.\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "duplicate_rows"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9f4465",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f9f4465",
    "outputId": "35d2d165-95cb-4371-d1b1-62d40b9acfda"
   },
   "source": [
    "# Identify missing values\n",
    "#The code is calculating the number of missing values in each column of the DataFrame `df`. It uses the `isna()` method to check for missing values and the `sum()` method to calculate the total number of missing values in each column. The result is stored in the variable `missing_values`.\n",
    "missing_values = df.isna().sum()\n",
    "missing_values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e841e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "74e841e9",
    "outputId": "b82ffaed-4050-456a-d169-117266994e63"
   },
   "source": [
    "# removing duplicates\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# Remove the duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df['tweets'] = df['tweets'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "stopw = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stopw)\n",
    "df[\"tweets\"] = df['tweets'].apply(lambda x: ' '.join(\n",
    "    [word.lower() for word in x.split() if word.lower() not in stopw]))\n",
    "\n",
    "\n",
    "def convert_list_to_str(l):\n",
    "    st = \"\"\n",
    "    for i in l:\n",
    "        st = st+i+\" \"\n",
    "    st = st[:-1]\n",
    "    return st\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\d+')\n",
    "df[\"tweets_new\"] = df[\"tweets\"].apply(tokenizer.tokenize)\n",
    "df[\"tweets_new\"] = df[\"tweets_new\"].apply(convert_list_to_str)\n",
    "\n",
    "\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91af7f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "c91af7f4",
    "outputId": "dfca058d-ceea-4530-b92d-d31443784ee7"
   },
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no mapping found\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # words = word_tokenize(sentence)\n",
    "    words = sentence.split()\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(\n",
    "        word, get_wordnet_pos(pos_tag)) for word, pos_tag in pos_tags]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "df[\"tweets_new\"] = df[\"tweets_new\"].apply(lemmatize_sentence)\n",
    "print(df)\n",
    "\n",
    "df[\"tweets\"] = df[\"tweets_new\"]\n",
    "\n",
    "# df[\"tweets\"] = df[\"tweets_new\"].apply(convert_list_to_str)\n",
    "df = df.drop(columns=[\"tweets_new\"])\n",
    "# print(df)\n",
    "\n",
    "#fill 0 in mum\n",
    "df['tweets'] = df['tweets'].fillna(0)\n",
    "\n",
    "# Save the dataframe\n",
    "df.to_csv(\"tweet_clean.csv\", index=False)\n",
    "df\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4b79cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef4b79cc",
    "outputId": "32460b8d-58c7-4f40-fc00-74edc9e44464"
   },
   "source": [
    "#check for duplicates\n",
    "df.duplicated().sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a392a91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a392a91",
    "outputId": "5eb4eed7-6142-4675-a617-244746749fe1"
   },
   "source": [
    "#check null\n",
    "df['tweets'].isnull().sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0609e972",
   "metadata": {
    "id": "0609e972"
   },
   "source": [
    "# Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb76673b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "cb76673b",
    "outputId": "5276e797-b459-4dac-a1b6-d79d43aedb31"
   },
   "source": [
    "#`df.describe().T` is transposing the output of the `describe()` method on a DataFrame `df`. The `describe()` method provides summary statistics of the numerical columns in the DataFrame, such as count, mean, standard deviation, minimum, maximum, and quartiles. By applying `.T` after `describe()`, the output is transposed, meaning the rows become columns and vice versa. This can be useful for better readability or for further analysis of the summary statistics.\n",
    "df.describe().T"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006b4366",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "006b4366",
    "outputId": "df61141a-53b3-4e50-bb65-5f6cafcccaee"
   },
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='sentiments', data=df)\n",
    "for container in plt.gca().containers:\n",
    "    plt.gca().bar_label(container, fmt='%.2f')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4cc280",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1a4cc280",
    "outputId": "b991e005-0e1d-42e8-e88b-0b161183a4e0"
   },
   "source": [
    "\n",
    "\n",
    "df_copy = df.copy()\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Every single column with categorical values will be converted.\n",
    "object_cols = ['sentiments']\n",
    "df_copy[object_cols] = df_copy[object_cols].astype(str)\n",
    "\n",
    "df_copy[object_cols] = ordinal_encoder.fit_transform(df_copy[object_cols])\n",
    "\n",
    "df_copy.head()\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Every single column with categorical values will be converted.\n",
    "object_cols = ['tweet_id', 'sentiments', 'tweets']\n",
    "df[object_cols] = df[object_cols].astype(str)\n",
    "\n",
    "df[object_cols] = ordinal_encoder.fit_transform(df[object_cols])\n",
    "\n",
    "df.head()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf40ce1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bf40ce1",
    "outputId": "e09ef62b-a223-4c75-e6c7-1e489486902e"
   },
   "source": [
    "print(df['sentiments'].nunique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "88771d80",
   "metadata": {
    "id": "88771d80"
   },
   "source": [
    "# download transformers and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9319eba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9319eba",
    "outputId": "f0dc0c24-22fb-4e84-f319-66a5f2c7f2f9"
   },
   "source": [
    "!pip install transformers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe9afbbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe9afbbc",
    "outputId": "29e1aaa7-8a1d-47d2-cc06-58b385a4a473"
   },
   "source": [
    "!pip install tensorflow"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT IMPLEMENTATION AND TRAINING"
   ],
   "id": "5c13908894562f35"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d16d8dac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d16d8dac",
    "outputId": "4a3dd6b6-4786-4944-8914-0ae73d2d28fc"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_df, test_df = train_test_split(df_copy, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define labels as a list of sentiment labels from your DataFrame\n",
    "train_labels = train_df['sentiments'].values.tolist()\n",
    "test_labels = test_df['sentiments'].values.tolist()\n",
    "\n",
    "\n",
    "num_labels = len(np.unique(train_labels))\n",
    "\n",
    "# Instantiate the BERT model and tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "max_seq_lengths = config.max_position_embeddings\n",
    "embedding_dims = config.hidden_size\n",
    "\n",
    "print(\"Max Sequence Length:\", max_seq_lengths)\n",
    "print(\"Embedding Dimension:\", embedding_dims)\n",
    "\n",
    "\n",
    "# precision and recall\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "# Prepare the inputs for the training set\n",
    "train_input_ids, train_attention_masks, train_encoded_labels = [], [], []\n",
    "\n",
    "for tweet, label in zip(train_df['tweets'], train_labels):\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
    "                                   return_attention_mask=True, return_token_type_ids=True)\n",
    "    train_input_ids.append(inputs['input_ids'])\n",
    "    train_attention_masks.append(inputs['attention_mask'])\n",
    "    train_encoded_labels.append(label)\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_input_ids = tf.convert_to_tensor(train_input_ids)\n",
    "train_attention_masks = tf.convert_to_tensor(train_attention_masks)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_encoded_labels = le.fit_transform(train_encoded_labels)\n",
    "\n",
    "\n",
    "train_encoded_labels = to_categorical(train_encoded_labels)\n",
    "\n",
    "# Prepare the inputs for the testing set\n",
    "test_input_ids, test_attention_masks, test_encoded_labels = [], [], []\n",
    "\n",
    "for tweet, label in zip(test_df['tweets'], test_labels):\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
    "                                   return_attention_mask=True, return_token_type_ids=True)\n",
    "    test_input_ids.append(inputs['input_ids'])\n",
    "    test_attention_masks.append(inputs['attention_mask'])\n",
    "    test_encoded_labels.append(label)\n",
    "\n",
    "\n",
    "test_input_ids = tf.convert_to_tensor(test_input_ids)\n",
    "test_attention_masks = tf.convert_to_tensor(test_attention_masks)\n",
    "\n",
    "\n",
    "test_encoded_labels = le.transform(test_encoded_labels)\n",
    "\n",
    "# Convert integer labels to one-hot encoded format\n",
    "test_encoded_labels = to_categorical(test_encoded_labels)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# train_embeddings = model.predict([train_input_ids, train_attention_masks])\n",
    "# test_embeddings = model.predict([test_input_ids, test_attention_masks])\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "926d2123",
   "metadata": {
    "id": "926d2123"
   },
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[precision, recall, 'accuracy'])\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1c9e9fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1c9e9fd",
    "outputId": "0605f6c3-f311-4214-9b10-ef33370181cc"
   },
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x=[train_input_ids, train_attention_masks],\n",
    "    y=train_encoded_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "# train_embeddings = model.predict([train_input_ids, train_attention_masks])\n",
    "# test_embeddings = model.predict([test_input_ids, test_attention_masks])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23ccbac9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23ccbac9",
    "outputId": "42be2051-2b24-40e3-e31d-baabeef69419"
   },
   "source": [
    "# Print the history\n",
    "print(history.history)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30e9d81e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30e9d81e",
    "outputId": "e0a4b2a7-bdf7-4996-ec92-5ff3a39b65ac"
   },
   "source": [
    "# Assuming 'history' is the History object returned by model.fit()\n",
    "history_dict = history.history\n",
    "\n",
    "# Print the training accuracy\n",
    "print(\"Accuracy: \", history_dict['accuracy'][-1])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Accuracy"
   ],
   "id": "b88baea534dd85ab"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7lDIRrx1TLMB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lDIRrx1TLMB",
    "outputId": "40d19bff-89a0-4c0d-8c0c-433d384af209"
   },
   "source": [
    "test_embeddings = model.predict([test_input_ids, test_attention_masks])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming test_encoded_labels are one-hot encoded labels \n",
    "# Assuming test_embeddings are the model predictions on the test data\n",
    "\n",
    "# Convert one-hot encoded labels back to integers\n",
    "test_predicted_labels = tf.argmax(test_embeddings.logits, axis=1).numpy()\n",
    "test_true_labels = tf.argmax(test_encoded_labels, axis=1).numpy()\n",
    "\n",
    "# Calculate  accuracy\n",
    "accuracy = accuracy_score(test_true_labels, test_predicted_labels)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "IdoS6mRgDb_u",
   "metadata": {
    "id": "IdoS6mRgDb_u"
   },
   "source": [
    "# model.save_weights('bert_model.weights')\n",
    "# model.save('bert_model.json')\n",
    "# print(\"Model Saved\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data preprocessing"
   ],
   "id": "95f9d1ca180337f0"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "jIBNv2t0DhMY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIBNv2t0DhMY",
    "outputId": "e2653f9d-216d-4728-fd04-daced6f0036d"
   },
   "source": [
    "test_data = pd.read_csv(\"twitter_validation.csv\")\n",
    "test_data.columns = [\"tweet_id\",\"place\", \"sentiments\", \"tweets\"]\n",
    "test_data = test_data.drop(\"place\", axis=1)\n",
    "\n",
    "test_data.drop_duplicates(inplace = True)\n",
    "\n",
    "# Remove the duplicate rows\n",
    "test_data = test_data.drop_duplicates()\n",
    "\n",
    "test_data['tweets'] = test_data['tweets'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "stopw = set(stopwords.words(\"english\"))\n",
    "\n",
    "print(stopw)\n",
    "test_data[\"tweets\"] = test_data['tweets'].apply(lambda x: ' '.join(\n",
    "    [word.lower() for word in x.split() if word.lower() not in stopw]))\n",
    "\n",
    "\n",
    "def convert_list_to_str(l):\n",
    "    st = \"\"\n",
    "    for i in l:\n",
    "        st = st+i+\" \"\n",
    "    st = st[:-1]\n",
    "    return st\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\d+')\n",
    "test_data[\"tweets_new\"] = test_data[\"tweets\"].apply(tokenizer.tokenize)\n",
    "test_data[\"tweets_new\"] = test_data[\"tweets_new\"].apply(convert_list_to_str)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no mapping found\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # words = word_tokenize(sentence)\n",
    "    words = sentence.split()\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_words = [lemmatizer.lemmatize(\n",
    "        word, get_wordnet_pos(pos_tag)) for word, pos_tag in pos_tags]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "test_data[\"tweets_new\"] = test_data[\"tweets_new\"].apply(lemmatize_sentence)\n",
    "# print(test_data)\n",
    "\n",
    "test_data[\"tweets\"] = test_data[\"tweets_new\"]\n",
    "\n",
    "# df[\"tweets\"] = df[\"tweets_new\"].apply(convert_list_to_str)\n",
    "test_data = test_data.drop(columns=[\"tweets_new\"])\n",
    "# print(df)\n",
    "\n",
    "#fill 0 in mum\n",
    "test_data['tweets'] = test_data['tweets'].fillna(0)\n",
    "\n",
    "# Save the dataframe\n",
    "test_data.to_csv(\"tweet_clean.csv\", index=False)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "gvbyS2CjKqSn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "gvbyS2CjKqSn",
    "outputId": "caeee2af-a8d9-4efa-ffe6-b9c2e9aafe04"
   },
   "source": [
    "print(test_data.head())\n",
    "td_copy = test_data.copy()\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Every single column with categorical values will be converted.\n",
    "object_cols = ['sentiments']\n",
    "td_copy[object_cols] = td_copy[object_cols].astype(str)\n",
    "\n",
    "td_copy[object_cols] = ordinal_encoder.fit_transform(td_copy[object_cols])\n",
    "\n",
    "td_copy.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ZUCZ8nSddKmY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUCZ8nSddKmY",
    "outputId": "5fb0eff4-2199-4541-f54a-6b2c745b1dcf"
   },
   "source": [
    "print(train_df.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "kyLzvUnHgUj1",
   "metadata": {
    "id": "kyLzvUnHgUj1"
   },
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Evaluation"
   ],
   "id": "15817f1639974c8a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "XBm-QuUCFqMA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBm-QuUCFqMA",
    "outputId": "e480fa86-90b4-4fd6-ef5a-0eaa59783a9f"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_df, test_df = df_copy, td_copy\n",
    "\n",
    "# Define labels as a list of sentiment labels from your DataFrame\n",
    "train_labels = train_df['sentiments'].values.tolist()\n",
    "test_labels = test_df['sentiments'].values.tolist()\n",
    "\n",
    "\n",
    "num_labels = len(np.unique(train_labels))\n",
    "\n",
    "# Instantiate the BERT model and tokenizer\n",
    "# model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "max_seq_lengths = config.max_position_embeddings\n",
    "embedding_dims = config.hidden_size\n",
    "\n",
    "print(\"Max Sequence Length:\", max_seq_lengths)\n",
    "print(\"Embedding Dimension:\", embedding_dims)\n",
    "\n",
    "\n",
    "\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "# Prepare the inputs for the testing set\n",
    "test_input_ids, test_attention_masks, test_encoded_labels = [], [], []\n",
    "\n",
    "for tweet, label in zip(test_df['tweets'], test_labels):\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
    "                                   return_attention_mask=True, return_token_type_ids=True)\n",
    "    test_input_ids.append(inputs['input_ids'])\n",
    "    test_attention_masks.append(inputs['attention_mask'])\n",
    "    test_encoded_labels.append(label)\n",
    "\n",
    "\n",
    "test_input_ids = tf.convert_to_tensor(test_input_ids)\n",
    "test_attention_masks = tf.convert_to_tensor(test_attention_masks)\n",
    "\n",
    "# Convert integer labels to one-hot encoded format\n",
    "test_encoded_labels = to_categorical(test_encoded_labels)\n",
    "\n",
    "# Predict using the model on the test data\n",
    "test_embeddings = model.predict([test_input_ids, test_attention_masks])\n",
    "\n",
    "# model evaluate\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[precision, recall, 'accuracy'])\n",
    "\n",
    "# Evaluate the model\n",
    "print(model.evaluate([test_input_ids, test_attention_masks], test_encoded_labels))\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "SbYVHWgmOSor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbYVHWgmOSor",
    "outputId": "bac05bcd-475a-4173-d429-fb9b91fa47a6"
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming test_encoded_labels are one-hot encoded labels (as in the previous example)\n",
    "# Assuming test_embeddings are the model predictions on the test data\n",
    "\n",
    "# Convert one-hot encoded labels back to integers\n",
    "test_predicted_labels = tf.argmax(test_embeddings.logits, axis=1).numpy()\n",
    "test_true_labels = tf.argmax(test_encoded_labels, axis=1).numpy()\n",
    "\n",
    "# Calculate Test Data accuracy\n",
    "accuracy = accuracy_score(test_true_labels, test_predicted_labels)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kAjNWYc3TCCv",
   "metadata": {
    "id": "kAjNWYc3TCCv"
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
